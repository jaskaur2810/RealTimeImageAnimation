# ðŸŒ€ Real-Time Image Animation using Deep Learning

This project brings static images to life by animating them using motion patterns from a driving video, powered by deepfake generation techniques. Based on the **First Order Motion Model for Image Animation**, it uses advanced deep learning architectures like **ResNext CNN** and **LSTM** to create realistic facial animations.

## ðŸ“Œ Features
- Real-time animation of any portrait image using a video of a real person's facial expressions.
- No 3D modeling or manual rigging required.
- Works with any input image and driving video.
- Extensible for applications in entertainment, virtual avatars, and deepfake detection.

## ðŸ§  Tech Stack
- **Python**
- **PyTorch**
- **OpenCV**, **FFmpeg**
- **ResNext CNN** for feature extraction
- **LSTM** for temporal consistency
- **VS Code**, Google Colab / Local Runtime

## ðŸ“‚ Project Structure
Real_Time_Image_Animation/
â”œâ”€â”€ checkpoints/                 # Pre-trained model weights (.pth files)
â”‚   â””â”€â”€ <model_name>.pth
â”‚
â”œâ”€â”€ config/                      # Configuration files (YAML)
â”‚   â””â”€â”€ <model_config>.yaml
â”‚
â”œâ”€â”€ demo.py                      # Main script to run the animation
â”œâ”€â”€ animate.py                   # Animation logic (may be called by demo.py)
â”œâ”€â”€ reconstruction.py            # For reconstructing videos (optional)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ inputs/                      # Input assets
â”‚   â”œâ”€â”€ source.png               # Static image (e.g., Mona Lisa)
â”‚   â””â”€â”€ driving.mp4              # Driving video with facial motion
â”‚
â”œâ”€â”€ outputs/                     # Output folder for generated videos
â”‚   â””â”€â”€ result.mp4
â”‚
â”œâ”€â”€ modules/                     # Core model modules
â”‚   â”œâ”€â”€ generator.py             # Generator network
â”‚   â”œâ”€â”€ keypoint_detector.py     # Keypoint detection logic
â”‚   â””â”€â”€ ...                      # Other model components
â”‚
â”œâ”€â”€ README.md                    # Project documentation
â””â”€â”€ LICENSE                      # License file (MIT recommended)

## ðŸš€ How It Works
1. **Input** a static source image and a driving video.
2. The model extracts **key facial landmarks** and motion vectors from the driving video.
3. Motion is **transferred to the static image**, animating it with realistic expressions and movements.
4. The **Generator Network** synthesizes a video where the image mimics the motion in the driving video.
